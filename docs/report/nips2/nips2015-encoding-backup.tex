\documentclass{article} % For LaTeX2e

\usepackage{nips15submit_e,times}
\usepackage{hyperref}
\usepackage{url}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{graphicx}
% \documentstyle[nips14submit_09,times,art10]{article} % For LaTeX 2.09


\title{Emotion classification based on logistic regression}


\author{
David S.~Hippocampus\thanks{ Use footnote for providing further information
about author (webpage, alternative address)---\emph{not} for acknowledging
funding agencies.} \\
Department of Computer Science\\
Cranberry-Lemon University\\
Pittsburgh, PA 15213 \\
\texttt{hippo@cs.cranberry-lemon.edu} \\
\And
Coauthor \\
Affiliation \\
Address \\
\texttt{email} \\
\AND
Coauthor \\
Affiliation \\
Address \\
\texttt{email} \\
\And
Coauthor \\
Affiliation \\
Address \\
\texttt{email} \\
\And
Coauthor \\
Affiliation \\
Address \\
\texttt{email} \\
(if needed)\\
}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\nipsfinalcopy % Uncomment for camera-ready version

\begin{document}


\maketitle

\begin{abstract}
In this assignment we try to predict the emotion of a face using basic machine learning techniques. We use principle component analysis to reduce the dimensionality of images. We then use reduced images as input to perform two-class classification using logistic regression. Average  prediction accuracy of a properly tuned model is around 70-90\%, depending on the specific pair of emotions that are desired to discern. We further use softmax regression to perform multi-class classification. Average prediction accuracy in this case is around 60-70\%. 
\end{abstract}

\section{Dataset and preprocessing}
The dataset contains the faces of 10 people, each showing 8 different emotions, namely happy, happy with teeth, maudlin, surprise, fear, anger, disgust and neutral. Our main goal is to train a classifier that predicts the emotions indicated by these faces.

To prepare the data for training, we pick out the faces of each subject according to the names of the image files, and store the images, the corresponding emotions and subject names in a tuple. In  this way we can select the faces of different subjects in the training part. 

\subsection{Examples of images}
As a glimpse of the image set, we show six different emotions of one subject in Fig. \ref{fig:example}.

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{../img/example}\caption{Sample images that show six different emotions of one subject}
\label{fig:example}
\end{figure}

\subsection{Dimension reduction using PCA}
The images are in the shape of (380, 240), that is 91200 features in total.
Given the large number of features, we have to reduce the dimensionality to gain efficiency as well as performance in training. We use principal component analysis to perform the reduction. To directly seek the singular vectors of the image data would be way more expensive. To do this efficiently, we use a little trick to derive the eigenvectors from a much smaller covariance matrix. Say $\boldsymbol{X}$ is the image data matrix in the shape of ($n$, $d$), where $n$ is the sample size, which is typically small, and $d$ is the number of features, which is 91200 in this case. The eigenvector factorization of the covariance matrix of $\boldsymbol{X}$ is
\begin{equation}
   \label{cov}
	\boldsymbol{X}^{T}\boldsymbol{X} \boldsymbol{W} = \lambda \boldsymbol{W},
\end{equation}
where $\boldsymbol{\Sigma} = \boldsymbol{X}^{T}\boldsymbol{X}$ is the covariance matrix of $\boldsymbol{X}$, and $\boldsymbol{W}$ is the eigenvector matrix, $\lambda$ is the eigenvalue matrix. Since $\boldsymbol{\Sigma}$ is in the shape of (91200, 91200), directly solving the eigenvectors in the above equation is barely viable. The trick then comes that we derive the covariance matrix in the other way, namely
\begin{equation}
	\boldsymbol{X} \boldsymbol{X}^{T}\boldsymbol{W'} = \lambda \boldsymbol{W'},
\end{equation}
where the covariance matrix $\boldsymbol{\Sigma'} = \boldsymbol{X}\boldsymbol{X}^T$ is in the shape of (n, n). Given the small sample size, this is a much smaller matrix. We then multiple $\boldsymbol{X}^T$ to both sides of the above equation, which becomes
\begin{equation}
	\boldsymbol{X}^{T} \boldsymbol{X} \boldsymbol{X}^{T}\boldsymbol{W'} = \boldsymbol{X}^{T} \lambda \boldsymbol{W'}.
\end{equation}
And it can be write alternatively as
\begin{equation}
	\boldsymbol{X}^{T} \boldsymbol{X} \boldsymbol{X}^{T}\boldsymbol{W'} =  \lambda \boldsymbol{X}^{T}\boldsymbol{W'}.
\end{equation}
Compared to Eq. (\ref{cov}), we find that
\begin{equation}
	\boldsymbol{W} = \boldsymbol{X}^{T}\boldsymbol{W'},
\end{equation}
which relates $\boldsymbol{W'}$ to $\boldsymbol{W}$. Since $\boldsymbol{W'}$ can be solve easily, we then gain great efficiency in calculating the eigenvectors of covariance matrix of image data. Notice that $\boldsymbol{W}$ calculated in this way must be normalized to serve as eigenvectors.

Given the eigenvectors, we can now reduce the dimensionality of the image data by
\begin{equation}
	\boldsymbol{X'} = \boldsymbol{X}\boldsymbol{W},
\end{equation}
where we can choose the number of principle components to control the dimensionality of reduced data. Notice that the number of principle components must be low than the sample size and the dimensionality of the original image, whichever the smaller.

\subsection{Visualize the eigenvectors}
Since each eigenvector has the same length of original dimensionality of images, we can reshape it to the image shape. Fig. \ref{fig:eigen} shows the pattern of the first 6 eigenvectors, where we perform PCA on the entire image data set, and choose 10 principle components.


\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{../img/eigenvectors}\caption{Sample eigenvectors derived from the data set.}
\label{fig:eigen}
\end{figure}

\section{Two-class classification}
Next we use logistic regression to classify the images. First and the easiest, we build a classifier to discern between two emotions, i.e. happy and maudlin in this case.

\subsection{Logistic regression}
The logistic regression is a binary classification technique. Intuitively, given a $d$-dimensional input vector $x\in \mathbb{R}^d$, it outputs a label $y$ between 0 and 1, which serves the estimate of the probability that the input is in the target category. The model is parameterized by a set of weights in the same dimension of the input, and an additional bias term, thus $w \in \mathbb{R}^{d+1}$. Typically, the activation function of logistic regression is a sigmoid function. Hence the entire model can be represented as
\begin{equation}
	y = P(\mathcal{C}_1|x) = g(w^Tx) = \frac{1}{1+\exp(-w^Tx)},
\end{equation}
where we should add an additional dimension that are all ones to the input matrix $x$.

The objective function in logistic regression is the cross-entropy function
\begin{equation}
	E(w) = -\sum_{n=1}^N \{t^n\log(y^n) + (1-t^n)\log(1-y^n)\},
\end{equation}
where $t^n\in\{0,1\}$ is the label indicate which category the input belongs to. We train the classifier by minimizing this objective function using gradient descent, that is we update the weight in the gradient direction of the loss function, scaled by a factor $\alpha$ called learning rate. One can prove that the gradient of cross-entropy function is
\begin{equation}
	-\frac{\partial E(w)^n}{\partial w_j} = (t^n - y^n) x_j^n,
\end{equation}
while the update rule is
\begin{equation}
	w \leftarrow w - \alpha \sum_n \nabla E^n(w).
\end{equation}

In a two-class classification problem we only need one output unit, because the output could either be 0 or 1, corresponding to two categories respectively. Hence, a simple logistic regression model is suitable to our problem.

\subsection{Training and evaluation}
We split the data set into three parts, i.e. train, holdout (validation) and test set, in a per-subject basis. The fraction are 80\%, 10\%, 10\% respectively. The holdout set is used to adjust the hyper-parameters of the model, and we will evaluate our model on the test set. We will train the model for 10 epochs, while within each epoch we let the entire train set through the pipeline, and update the weight once. This is known as the batch gradient descent. Due to the small sample size, we use a manifold technique, namely running the entire model for multiple times, and each time choosing the faces of a different subject as test set, while the train and holdout set are picked arbitrarily. In this case since we have 10 different subjects, we will run the model for 10 times, and evaluate the model averagelly. To capture the best performance during training, we maintain a variable to store the best weights ever reached, according to the loss on the holdout set. This is known as early stopping.

\subsubsection{Learning curve}
In Fig. \ref{fig:error} we plot the loss function over 10 epochs, where the loss on each epoch is the average value of 10 runs, and an error bar shows the standard deviation of this loss over 10 runs. We show the training and holdout loss in the same figure. One can find that both the training and holdout loss drop as training goes, while training loss drops more rapidly. 

\begin{figure}[h]
\centering
\includegraphics[width=0.6\textwidth]{../img/lR_error}\caption{The training and holdout loss over 10 epochs. On each data point the loss is averaged over 10 runs, and the standard deviation is shown as an error bar respectively.}
\label{fig:error}
\end{figure}

\subsubsection{Test accuracy}
We evaluate the model on the test set, and will average the accuracy over ten runs since we are using a manifold technique. Typically the fraction of test faces that are predicted correctly is about $95.0 \% (15.0)$, if hyper-parameters are properly chosen.

\subsubsection{Effect of the learning rate}
Different learning rates may affect the performance of the model drastically. In Fig. \ref{fig:error_lr} we show the training loss over training epochs for different learning rate. One can see that for a small learning rate the loss decreases quite slowly, while for a large learning rate it goes the other way. However, a learning rate that is too large may cause great instability to training. Though the training loss always decreases, the accuracy of the model on the test set may not be acceptable. Hence a proper learning rate should be in between. In the above result we choose a learning rate that is 0.1.

\begin{figure}[h]
\centering
\includegraphics[width=0.6\textwidth]{../img/lR_error_lrs}\caption{The training loss over training epochs for different learning rate. The learning rate attached to each curve is indicated in the legend.}
\label{fig:error_lr}
\end{figure}

\subsubsection{Effect of the dimensionality}
Recall that we use PCA to reduce the dimensionality of the data. The number of dimensions that are left is a hyper-parameter that we can tune. In Fig. \ref{fig:error_lr} we choose three number of components, and show the training loss over training epochs. One can find that for an extremely small number of components, e.g. only one dimension, the model basically fails to reduce the loss during training, obviously because the model is too simple. On the other hand for a large number of components the model behaves well. Since we are using a simple logistic regression model, more weights incorporated in the model generally leads to high capability.

\begin{figure}[h]
\centering
\includegraphics[width=0.6\textwidth]{../img/lR_error_component}\caption{The training loss over training epochs for different number of principal components, or the dimensionality of data after reduction. The number of components attached to each curve is indicated in the legend.}
\label{fig:error_lr}
\end{figure}

\subsection{Evaluate on a different pair of emotions}
In the above we try to discern happy faces from maudlin ones. Utilizing the exact same model, we can try to discern different emotions. For example, we pick afraid and surprised faces here as data set, and train a logistic regression model to classify these two emotions. We should choose the same hyper-parameters as above here.

\subsubsection{cross-entropy loss}
Using the best learning rate found above, i.e. 0.1, we train the model similarly. In Fig. \ref{fig:error2} we show the training and holdout loss over training epochs. Same as above, we will run the model for 10 times and report the average and standard deviation. 

\begin{figure}[h]
\centering
\includegraphics[width=0.6\textwidth]{../img/lR_error_fs}\caption{The training and holdout loss over 10 epochs. On each data point the loss is averaged over 10 runs, and the standard deviation is shown as an error bar respectively.}
\label{fig:error2}
\end{figure}

\subsubsection{Test accuracy}
The test accuracy averaged over the 10 runs is $75.0 \% (25)$.

\subsubsection{Difference}
The accuracy that model achieved on classifying afraid and surprise faces are lower than that on happy and maudlin faces. Possible reason is that afraid and surprise faces are more hard to discern. For example, typical afraid and surprise faces both comprise open mouths and wide eyes. These are basically the same regarding the main structures of faces, which makes it hard to classify. On the other hand, happy and maudlin faces are significantly different in eyes and mouths.


\section{Multi-class classification}
Softmax regression is for multi-class classification problems. Given an input $x^n$, the probability that $x^n$ is in class $k$ is written as
\begin{equation}
	y^n_k=\frac{exp(w^T_kx^n)}{\sum_{k`}exp(w^T_{k'}x^n) }
\end{equation}

We use one-hot to encode target $t^n$, so we can deÔ¨Åne the cross-entropy cost funtion as 
\begin{equation}
	E=-\sum\limits_{n}\sum\limits_{k=1}^{c} t^n_klny^n_k 
\end{equation}

With $w_{jk}$ to be the weight from $j^{th}$ input to the $k^{th}$ output, the gradient can be compute as
\begin{equation}
\frac{\partial E^n(w)}{\partial w_{jk}}=(y^n_k-t^n_k)x^n_j
\end{equation}


\subsection{Multi-class classification on all six emotions}
Only use one happy face for each object to maintain the balance between categories. If we use all the happy faces, the data will be biased and it will be more likely to recognize a face to be happy. Run the test 10 times, each time using a different object as test data. Set the maximum epochs to 50.


\subsubsection{Learning curve}
After training 10 times, plot the average value of the training loss and the holdout loss at each epoch with standard deviation at 10, 20, 30, 40, 50 epochs.

\begin{figure}[h]
\centering
\includegraphics[width=0.6\textwidth]{../img/soft_error}\caption{The training and holdout loss over training epochs. On each data point the loss is averaged over 10 runs, and the standard deviation is shown as an error bar respectively.}
\label{fig:soft_error}
\end{figure}

from the plot we can see that as we train more epochs, the loss of both
training set and validation set is gradually decreasing and converge to some final value. However, there is a visible gap between training loss and validation loss, which indicates that the system may be overfitting.

\subsubsection{Confusion matrix}

\begin{figure}[h]
\centering
\includegraphics[width=0.6\textwidth]{../img/confusion}\caption{The confusion matrix. `h', `m', `a', `s', `d', `f' indicate `happy', `maudlin', `anger', `surprise', `disgust' and `fear' respectively. Each row indicate the true label. Each column indicate the predicted label. Thus the fraction at  $i$-th row and $j$-th column represent the percent that the model classify the $i$-th emotion to the $j$-th emotion.}
\label{fig:confusion}
\end{figure}

\subsection{Batch versus stochastic gradient descent}
\subsubsection{Stochastic gradient descent}
\subsubsection{learning curve of two methods}

\begin{figure}[h]
\centering
\includegraphics[width=0.6\textwidth]{../img/soft_error_mode}\caption{The training loss over training epochs for different training modes, i.e. batch and stochastic gradient descent. On each data point the loss is averaged over 10 runs, and the standard deviation is shown as an error bar respectively.}
\label{fig:soft_error_mode}
\end{figure}


\subsection{Visualize weights}
The weights achieved by the Softmax regression is in the shape of  ($d$,$c$), where $d$ is the number of features and $c$ is the number of features. The bases of PCA used to reduce image dimensionality are in the shape of (91200, $d$). Thus we can actually use the bases of PCA to transform the weights back to the image dimensionality. After reshaping we can plot these weights, which is shown in Fig. \ref{fig:weight}. The pattern of recovered weights is consistent with its emotion category, though ghost like. For example, the recovered weights associated with `happy' is basically a mixed face that capture the main characteristics of a happy face. In terms of the Softmax model, the weights associated with each category utilize all the features and determine whether this category should be activated. They should indeed reflect the basic structure of the corresponding emotions when recovered.

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{../img/weights}\caption{Visualization of the weights in the image shape.}
\label{fig:weight}
\end{figure}

\subsection{Classify the faces into identities}
\begin{figure}[h]
\centering
\includegraphics[width=0.6\textwidth]{../img/identities}\caption{Visualization of the weights achieved by a Softmax classifier trying to classify different identities.}
\label{fig:soft_error_mode}
\end{figure}


\section{Contribution}
Chengyu Dong wrote all the prototype code, including model construction and visualization. Junliang Yu helped debug and improve the code, as well as review related materials to make sure all requirements were met. 

In the report, Chengyu Dong wrote the PCA and Logistic Regression parts, and Junliang Yu wrote the Softmax Regression part. Chengyu Dong was also responsible for maintaining the \LaTeX file and improving the readability of the report.
%\subsubsection*{Acknowledgments}
%
%Use unnumbered third level headings for the acknowledgments. All
%acknowledgments go at the end of the paper. Do not include 
%acknowledgments in the anonymized submission, only in the 
%final paper. 

%\subsubsection*{References}
%
%References follow the acknowledgments. Use unnumbered third level heading for
%the references. Any choice of citation style is acceptable as long as you are
%consistent. It is permissible to reduce the font size to `small' (9-point) 
%when listing the references. {\bf Remember that this year you can use
%a ninth page as long as it contains \emph{only} cited references.}
%
%\small{
%[1] Alexander, J.A. \& Mozer, M.C. (1995) Template-based algorithms
%for connectionist rule extraction. In G. Tesauro, D. S. Touretzky
%and T.K. Leen (eds.), {\it Advances in Neural Information Processing
%Systems 7}, pp. 609-616. Cambridge, MA: MIT Press.
%
%[2] Bower, J.M. \& Beeman, D. (1995) {\it The Book of GENESIS: Exploring
%Realistic Neural Models with the GEneral NEural SImulation System.}
%New York: TELOS/Springer-Verlag.
%
%[3] Hasselmo, M.E., Schnell, E. \& Barkai, E. (1995) Dynamics of learning
%and recall at excitatory recurrent synapses and cholinergic modulation
%in rat hippocampal region CA3. {\it Journal of Neuroscience}
%{\bf 15}(7):5249-5262.
%}

\end{document}
